| repo                                                                      | star  | area   | about                                                                                                                                                                                                                                                                   |
|---------------------------------------------------------------------------|-------|--------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [langfuse](https://github.com/langfuse/langfuse)                          | 16.7k | å›½å¤–     | Open source LLM engineering platform: LLM Observability, metrics, evals, prompt management, playground, datasets. Integrates with OpenTelemetry, Langchain, OpenAI SDK, LiteLLM, and more. ğŸŠYC W23                                                                     |
| [opik](https://github.com/comet-ml/opik)                                  | 14.3k | å›½å¤–     | Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.                                                                                              |
| [deepeval](https://github.com/confident-ai/deepeval)                      | 11.3k | å›½å¤–     | The LLM Evaluation Framework                                                                                                                                                                                                                                            |
| [ragas](https://github.com/explodinggradients/ragas)                      | 10.9k | å›½å¤–     | Supercharge Your LLM Application Evaluations ğŸš€                                                                                                                                                                                                                         |
| [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) | 10.2k | å›½å¤–     | A framework for few-shot evaluation of language models.                                                                                                                                                                                                                 |
| [promptfoo](https://github.com/promptfoo/promptfoo)                       | 8.5k  | å›½å¤–     | Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.                                  |
| [phoenix](https://github.com/Arize-ai/phoenix)                            | 7.1k  | å›½å¤–     | AI Observability & Evaluation                                                                                                                                                                                                                                           |
| [opencompass](https://github.com/open-compass/opencompass)                | 6.1k  | **å›½å†…** | OpenCompass is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.                                                                                                       |
| [garak](https://github.com/NVIDIA/garak)                                  | 6.1k  | å›½å¤–     | the LLM vulnerability scanner                                                                                                                                                                                                                                           |
| [â­chinese-llm-benchmarkï¼ˆæˆ‘ä»¬ï¼‰](https://github.com/jeinlee1991/chinese-llm-benchmark) | 4.9k  | **å›½å†…** | ReLEä¸­æ–‡å¤§æ¨¡å‹èƒ½åŠ›è¯„æµ‹ï¼ˆæŒç»­æ›´æ–°ï¼‰ï¼šç›®å‰å·²å›Šæ‹¬288ä¸ªå¤§æ¨¡å‹ï¼Œè¦†ç›–chatgptã€gpt-5ã€o4-miniã€è°·æ­Œgemini-2.5ã€Claudeã€æ™ºè°±GLM-Z1ã€æ–‡å¿ƒä¸€è¨€ã€qwen-maxã€ç™¾å·ã€è®¯é£æ˜Ÿç«ã€å•†æ±¤senseChatã€minimaxç­‰å•†ç”¨æ¨¡å‹ï¼Œ ä»¥åŠDeepSeek-R1-0528ã€deepseek-v3ã€qwen3ã€llama4ã€glm4.5ã€gemma3ã€mistralç­‰å¼€æºå¤§æ¨¡å‹ã€‚ä¸ä»…æä¾›æ’è¡Œæ¦œï¼Œä¹Ÿæä¾›è§„æ¨¡è¶…200ä¸‡çš„å¤§æ¨¡å‹ç¼ºé™·åº“ï¼æ–¹ä¾¿å¹¿å¤§ç¤¾åŒºç ”ç©¶åˆ†æã€æ”¹è¿›å¤§æ¨¡å‹ã€‚ |                                                                                               |
| [ARC-AGI](https://github.com/fchollet/ARC-AGI)                            | 4.6k  | å›½å¤–     | The Abstraction and Reasoning Corpus                                                                                                                                                                                                                                                                        |
| [helicone](https://github.com/Helicone/helicone)                          | 4.5k  | å›½å¤–     | ğŸ§Š Open source LLM observability platform. One line of code to monitor, evaluate, and experiment. YC W23 ğŸ“                                                                                                                                                                                                                                                                       |
| [AutoRAG](https://github.com/Marker-Inc-Korea/AutoRAG)                    | 4.3k  | å›½å¤–     | AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation                                                                                                                                                                                                                                                                       |
| [simple-evals](https://github.com/openai/simple-evals)                    | 4.1k  | å›½å¤–     | This repository contains a lightweight library for evaluating language models. We are open sourcing it so we can be transparent about the accuracy numbers we're publishing alongside our latest models.                                                                                                                                                                                                                                                                       |
| [SWE-bench](https://github.com/SWE-bench/SWE-bench)                       | 3.6k  | å›½å¤–     | SWE-bench [Multimodal]: Can Language Models Resolve Real-world Github Issues?                                                                                                                                                                                                                                                                       |
| [SuperCLUE](https://github.com/CLUEbenchmark/SuperCLUE)                   | 3.3k  | **å›½å†…** | SuperCLUE: ä¸­æ–‡é€šç”¨å¤§æ¨¡å‹ç»¼åˆæ€§åŸºå‡† A Benchmark for Foundation Models in Chinese                                                                                                                                                                                                                                                                        |
| [agenta](https://github.com/Agenta-AI/agenta)                             | 3.2k  | å›½å¤–     | The open-source LLMOps platform: prompt playground, prompt management, LLM evaluation, and LLM observability all in one place.                                                                                                                                                                                                                                                                        |
| [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval)                | 3.1k  | å›½å¤–     | One-for-All Multimodal Evaluation Toolkit Across Text, Image, Video, and Audio Tasks                                                                                                                                                                                                                                                                        |
| [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)                  | 3.1k  | **å›½å†…** |  Open-source evaluation toolkit of large multi-modality models (LMMs), support 220+ LMMs, 80+ benchmarks                                                                                                                                                                                                                                                                       |
| [HumanEval](https://github.com/openai/human-eval)                         | 2.9k  | å›½å¤–     |  Code for the paper "Evaluating Large Language Models Trained on Code"                                                                                                                                                                                                                                                                       |
| [hallucination-leaderboard](https://github.com/vectara/hallucination-leaderboard) | 2.8k  | å›½å¤–     | Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents                                                                                                                                                                                                                                                                        |
| [trulens](https://github.com/truera/trulens)                              | 2.8k  | å›½å¤–     | Evaluation and Tracking for LLM Experiments and AI Agents                                                                                                                                                                                                                                                                        |
| [promptbench](https://github.com/microsoft/promptbench)                   | 2.7k  | å›½å¤–     | A unified evaluation framework for large language models                                                                                                                                                                                                                                                                        |
| [langwatch](https://github.com/langwatch/langwatch)                       | 2.5k  | å›½å¤–     | The open LLM Ops platform - Traces, Analytics, Evaluations, Datasets and Prompt Optimization âœ¨                                                                                                                                                                          |
| [evaluate](https://github.com/huggingface/evaluate)                       | 2.3k  | å›½å¤–     | ğŸ¤— Evaluate: A library for easily evaluating machine learning models and datasets.                                                                                                                                                                                                                                                                        |
| [lmnr](https://github.com/lmnr-ai/lmnr)                                   | 2.3k  | å›½å¤–     |Laminar - open-source all-in-one platform for engineering AI products. Create data flywheel for your AI app. Traces, Evals, Datasets, Labels. YC S24.|
| [lighteval](https://github.com/huggingface/lighteval)                     | 2.0k  | å›½å¤–     |Lighteval is your all-in-one toolkit for evaluating LLMs across multiple backends|
| [EvalAI](https://github.com/Cloud-CV/EvalAI)                              | 1.9k  | å›½å¤–     |EvalAI is an open source platform for evaluating and comparing machine learning (ML) and artificial intelligence (AI) algorithms at scale.|
| [alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)                   | 1.9k  | å›½å¤–     |An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast.|
| [CodeXGLUE](https://github.com/microsoft/CodeXGLUE)                       | 1.7k  | å›½å¤–     |CodeXGLUE, a benchmark dataset and open challenge for code intelligence. It includes a collection of code intelligence tasks and a platform for model evaluation and comparison.|
| [agentic_security](https://github.com/msoedov/agentic_security)           | 1.7k  | å›½å¤–     |Agentic LLM Vulnerability Scanner / AI red teaming kit ğŸ§ª|
| [evalscope](https://github.com/modelscope/evalscope)                      | 1.7k  | **å›½å†…** |A streamlined and customizable framework for efficient large model evaluation and performance benchmarking|
| [LLM-eval-survey](https://github.com/MLGroupJLU/LLM-eval-survey)          | 1.6k  | **å›½å†…**     |The official GitHub page for the survey paper "A Survey on Evaluation of Large Language Models".|
| [MMLU](https://github.com/hendrycks/test)                                 | 1.5k  | å›½å¤–     |Measuring Massive Multitask Language Understanding | ICLR 2021|
| [lunary](https://github.com/lunary-ai/lunary)                             | 1.4k  | å›½å¤–     |The production toolkit for LLMs. Observability, prompt management and evaluations.|
| [MATH](https://github.com/hendrycks/math)                                 | 1.2k  | å›½å¤–     |The MATH Dataset (NeurIPS 2021)|
| [prompty](https://github.com/microsoft/prompty)                           | 1.1k  | å›½å¤–     |Prompty makes it easy to create, manage, debug, and evaluate LLM prompts for your AI applications. Prompty is an asset class and format for LLM prompts designed to enhance observability, understandability, and portability for developers.|
| [Humanity's Last Exam](https://github.com/centerforaisafety/hle)          | 1.1k  | å›½å¤–     |Humanity's Last Exam (HLE) is a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. Humanity's Last Exam consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. |
| [judgeval](https://github.com/JudgmentLabs/judgeval)                      | 1.0k  | å›½å¤–     |The open source post-building layer for agents. Our traces + evals power agent post-training (RL, SFT), monitoring, and regression testing.|
| [uqlm](https://github.com/cvs-health/uqlm)                                | 1.0k  | å›½å¤–     |UQLM: Uncertainty Quantification for Language Models, is a Python package for UQ-based LLM hallucination detection|
| [LiveBench](https://github.com/LiveBench/LiveBench)                       | 0.9k  | å›½å¤–     |LiveBench: A Challenging, Contamination-Free LLM Benchmark|
| [AGIEval](https://github.com/ruixiangcui/AGIEval)                         | 0.8k  | å›½å¤–     |This repository contains information about AGIEval, data, code and output of baseline systems for the benchmark.|
| [TruthfulQA](https://github.com/sylinrl/TruthfulQA)                       | 0.8k  | å›½å¤–     |TruthfulQA: Measuring How Models Imitate Human Falsehoods|
| [tau-bench](https://github.com/sierra-research/tau-bench)                 | 0.8k  | å›½å¤–     |Ï„-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains|
| [CBLUE](https://github.com/CBLUEbenchmark/CBLUE)                          | 0.8k  | **å›½å†…**     |[CBLUE1] ä¸­æ–‡åŒ»ç–—ä¿¡æ¯å¤„ç†åŸºå‡†CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark|
| [CMMLU](https://github.com/haonan-li/CMMLU)                               | 0.8k  | **å›½å†…**     |CMMLU: Measuring massive multitask language understanding in Chinese|
| [FuzzyAI](https://github.com/cyberark/FuzzyAI)                            | 0.7k  | å›½å¤–     |A powerful tool for automated LLM fuzzing. It is designed to help developers and security researchers identify and mitigate potential jailbreaks in their LLM APIs.|
| [ GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench)                | 0.7k  | **å›½å†…**     |GAOKAO-Bench is an evaluation framework that utilizes GAOKAO questions as a dataset to evaluate large language models.|
| [LiveCodeBench](https://github.com/LiveCodeBench/LiveCodeBench)                                                         | 0.7k  | å›½å¤–     |Official repository for the paper "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"|

