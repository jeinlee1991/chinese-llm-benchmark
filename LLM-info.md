# ä¸­æ–‡å¤§æ¨¡å‹æ±‡æ€»
ğŸ“å¤§æ¨¡å‹åŸºæœ¬ä¿¡æ¯

| å¤§æ¨¡å‹                                                                                     | æœºæ„             | ç±»åˆ«    | å¤‡æ³¨                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-----------------------------------------------------------------------------------------|----------------|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [chatgpt-3.5](https://chat.openai.com/)                                                 | openai         | å•†ç”¨    | é£é¡ä¸–ç•Œçš„AIäº§å“ï¼ŒAPIä¸ºgpt3.5-turbo                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [gpt4](https://chat.openai.com/)                                                        | openai         | å•†ç”¨    | å½“å‰ä¸–ç•Œæœ€å¼ºAI                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| [new-bing](https://www.bing.com/)                                                       | å¾®è½¯             | å•†ç”¨  |bingæœç´¢ç”¨çš„èŠå¤©æ¨¡å‹ï¼ŒåŸºäºGPT4|
| [æ–‡å¿ƒä¸€è¨€](https://yiyan.baidu.com/)                                                        | ç™¾åº¦             | å•†ç”¨    | ç™¾åº¦å…¨æ–°ä¸€ä»£çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œæ–‡å¿ƒå¤§æ¨¡å‹å®¶æ—çš„æ–°æˆå‘˜ï¼Œèƒ½å¤Ÿä¸äººå¯¹è¯äº’åŠ¨ï¼Œå›ç­”é—®é¢˜ï¼ŒååŠ©åˆ›ä½œï¼Œé«˜æ•ˆä¾¿æ·åœ°å¸®åŠ©äººä»¬è·å–ä¿¡æ¯ã€çŸ¥è¯†å’Œçµæ„Ÿã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [chatglmå®˜æ–¹](https://chatglm.cn/)                                                        | æ™ºè°±AI           | å•†ç”¨    | ä¸€ä¸ªå…·æœ‰é—®ç­”ã€å¤šè½®å¯¹è¯å’Œä»£ç ç”ŸæˆåŠŸèƒ½çš„ä¸­è‹±åŒè¯­æ¨¡å‹ï¼ŒåŸºäºåƒäº¿åŸºåº§ GLM-130B å¼€å‘ï¼Œé€šè¿‡ä»£ç é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒç­‰æŠ€æœ¯æå‡å„é¡¹èƒ½åŠ›                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [è®¯é£æ˜Ÿç«](https://xinghuo.xfyun.cn/desk)                                                   | ç§‘å¤§è®¯é£           | å•†ç”¨    | å…·æœ‰æ–‡æœ¬ç”Ÿæˆã€è¯­è¨€ç†è§£ã€çŸ¥è¯†é—®ç­”ã€é€»è¾‘æ¨ç†ã€æ•°å­¦èƒ½åŠ›ã€ä»£ç èƒ½åŠ›ã€å¤šæ¨¡æ€èƒ½åŠ› 7 å¤§æ ¸å¿ƒèƒ½åŠ›ã€‚è¯¥å¤§æ¨¡å‹ç›®å‰å·²åœ¨æ•™è‚²ã€åŠå…¬ã€è½¦è½½ã€æ•°å­—å‘˜å·¥ç­‰å¤šä¸ªè¡Œä¸šå’Œäº§å“ä¸­è½åœ°ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [360æ™ºè„‘](https://ai.360.cn/)                                                             | å¥‡è™360          | å•†ç”¨    | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| [é˜¿é‡Œé€šä¹‰åƒé—®](https://tongyi.aliyun.com/)                                                    | é˜¿é‡Œå·´å·´           | å•†ç”¨    | é€šä¹‰åƒé—®æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯è¿›è¡Œæ–‡æ¡ˆåˆ›ä½œã€é€»è¾‘æ¨ç†ï¼Œæ”¯æŒå¤šç§è¯­è¨€ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [senseChat](https://chat.sensetime.com/)                                                | å•†æ±¤             |å•†ç”¨|å•†æ±¤æ¨å‡ºçš„èŠå¤©æ¨¡å‹|
| [minimax](https://api.minimax.chat/)                                                    | minimax        | å•†ç”¨    | Glow appèƒŒåå¤§æ¨¡å‹                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [tigerbot-7bå®˜ç½‘](https://www.tigerbot.com/)                                              | è™åšç§‘æŠ€           | å•†ç”¨/å¼€æº | TigerBot æ˜¯ä¸€ä¸ªå¤šè¯­è¨€å¤šä»»åŠ¡çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLM)ï¼ŒåŸºäºbloomæ¨¡å‹ç»“æ„ã€‚è¯¥æ¨¡å‹ä¹Ÿæœ‰å¼€æºç‰ˆæœ¬ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [chatglm-6b](https://github.com/THUDM/ChatGLM-6B)                                       | æ¸…åå¤§å­¦&æ™ºè°±AI      | å¼€æº    | ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº General Language Model (GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”                                                                                                                                                                                                                                                             |
| [belle-llama-7b-2m](https://github.com/LianjiaTech/BELLE)                               | é“¾å®¶ç§‘æŠ€           | å¼€æº    | based on LLAMA 7B and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities.                                                                                                                                                                                                                                                                                           |
| [BELLE-on-Open-Datasets](https://github.com/LianjiaTech/BELLE)                          | é“¾å®¶ç§‘æŠ€           | å¼€æº    | Extending the vocabulary with additional 50K tokens specific for Chinese and further pretraining these word embeddings on Chinese corpus. Full-parameter finetuning the model with  instruction-following open datasets: alpaca, sharegpt, belle-3.5m.                                                                                                                                                                                                                                                                     |
| [belle-llama-13b-2m](https://github.com/LianjiaTech/BELLE)                              | é“¾å®¶ç§‘æŠ€           | å¼€æº    | based on LLAMA 13B and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca.                                                                                                                                                                                                                                                                                                                                                                                    |
| [belle-llama-13b-ext](https://github.com/LianjiaTech/BELLE)                             | é“¾å®¶ç§‘æŠ€           | å¼€æº    | Extending the vocabulary with additional 50K tokens specific for Chinese and further pretraining these word embeddings on Chinese corpus. Full-parameter finetuning the model with 4M high-quality instruction-following examples.                                                                                                                                                                                                                                                                                         |
| [BELLE-Llama2-13B-chat-0.4M](https://huggingface.co/BELLE-2/BELLE-Llama2-13B-chat-0.4M) |é“¾å®¶ç§‘æŠ€|å¼€æº |This model is obtained by fine-tuning the complete parameters using 0.4M Chinese instruction data on the original Llama2-13B-chat. |
| [Ziya-LLaMA-13B-v1](https://mp.weixin.qq.com/s/IeXgq8blGoeVbpIlAUCAjA)                  | IDEAç ”ç©¶é™¢        | å¼€æº    | ä»LLaMA-13Bå¼€å§‹é‡æ–°æ„å»ºä¸­æ–‡è¯è¡¨ï¼Œè¿›è¡Œåƒäº¿tokené‡çº§çš„å·²çŸ¥çš„æœ€å¤§è§„æ¨¡ç»§ç»­é¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹å…·å¤‡åŸç”Ÿä¸­æ–‡èƒ½åŠ›ã€‚å†ç»è¿‡500ä¸‡æ¡å¤šä»»åŠ¡æ ·æœ¬çš„æœ‰ç›‘ç£å¾®è°ƒ(SFT)å’Œç»¼åˆäººç±»åé¦ˆè®­ç»ƒï¼ˆRM+PPO+HFFT+COHFT+RBRS)ï¼Œè¿›ä¸€æ­¥æ¿€å‘å’ŒåŠ å¼ºå„ç§AIä»»åŠ¡èƒ½åŠ›ã€‚                                                                                                                                                                                                                                                                                                                                                                                      |
| [Ziya-LLaMA-13B-v1.1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1)             | IDEAç ”ç©¶é™¢        | å¼€æº    | å¯¹Ziya-LLaMA-13B-v1æ¨¡å‹è¿›è¡Œç»§ç»­ä¼˜åŒ–ï¼Œé€šè¿‡è°ƒæ•´å¾®è°ƒæ•°æ®çš„æ¯”ä¾‹å’Œé‡‡ç”¨æ›´ä¼˜çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæœ¬ç‰ˆæœ¬åœ¨é—®ç­”å‡†ç¡®æ€§ã€æ•°å­¦èƒ½åŠ›ä»¥åŠå®‰å…¨æ€§ç­‰æ–¹é¢å¾—åˆ°äº†æå‡                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| [guanaco-7b](https://huggingface.co/JosephusCheung/Guanaco)                             | JosephusCheung | å¼€æº    | Guanaco is an advanced instruction-following language model built on Meta's LLaMA 7B model. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534K+ entries have been incorporated, covering English, Simplified Chinese, Traditional Chinese (Taiwan), Traditional Chinese (Hong Kong), Japanese, Deutsch, and various linguistic and grammatical tasks. This wealth of data enables Guanaco to perform exceptionally well in multilingual environments.                                        |
| [phoenix-inst-chat-7b](https://github.com/FreedomIntelligence/LLMZoo)                   | é¦™æ¸¯ä¸­æ–‡å¤§å­¦         | å¼€æº    | åŸºäºBLOOMZ-7b1-mtï¼Œç”¨Instruction + Conversationæ•°æ®å¾®è°ƒï¼Œå…·ä½“æ•°æ®è§[phoenix-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/phoenix-sft-data-v1)                                                                                                                                                                                                                                                                                                                                                                        |
| [linly-chatflow-13b](https://github.com/CVI-SZU/Linly)                                  | æ·±åœ³å¤§å­¦           | å¼€æº    | åŸºäºllama-13bï¼Œç”¨5M æŒ‡ä»¤æ•°æ®å¾®è°ƒ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [Linly-Chinese-LLaMA2-13B](https://huggingface.co/Linly-AI/Chinese-LLaMA-2-13B-hf)      |æ·±åœ³å¤§å­¦|å¼€æº|Linly-Chinese-LLaMA2 åŸºäº LLaMA2è¿›è¡Œä¸­æ–‡åŒ–è®­ç»ƒï¼Œä½¿ç”¨è¯¾ç¨‹å­¦ä¹ æ–¹æ³•è·¨è¯­è¨€è¿ç§»ï¼Œè¯è¡¨é’ˆå¯¹ä¸­æ–‡é‡æ–°è®¾è®¡ï¼Œæ•°æ®åˆ†å¸ƒæ›´å‡è¡¡ï¼Œæ”¶æ•›æ›´ç¨³å®šã€‚|
| [MOSS-003-SFT](https://github.com/OpenLMLab/MOSS)                                       | å¤æ—¦å¤§å­¦           | å¼€æº    | MOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚                                                                                                                                                                                                                                                                                                                                       |
| [AquilaChat-7B](https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README.md) | æ™ºæºç ”ç©¶é™¢          | å¼€æº    | æ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰ è¯­è¨€å¤§æ¨¡å‹æ˜¯é¦–ä¸ªå…·å¤‡ä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€å›½å†…æ•°æ®åˆè§„éœ€æ±‚çš„å¼€æºè¯­è¨€å¤§æ¨¡å‹ã€‚AquilaChat å¯¹è¯æ¨¡å‹æ”¯æŒæµç•…çš„æ–‡æœ¬å¯¹è¯åŠå¤šç§è¯­è¨€ç±»ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡å®šä¹‰å¯æ‰©å±•çš„ç‰¹æ®ŠæŒ‡ä»¤è§„èŒƒï¼Œå®ç° AquilaChatå¯¹å…¶å®ƒæ¨¡å‹å’Œå·¥å…·çš„è°ƒç”¨ï¼Œä¸”æ˜“äºæ‰©å±•ã€‚                                                                                                                                                                                                                                                                                                                                                                                 |
| [tulu-30b](https://github.com/allenai/open-instruct)                                    | allenai        | å¼€æº    | We explore instruction-tuning popular base models on publicly available datasets. As part of this work we introduce TÃ¼lu, a suite of LLaMa models fully-finetuned on a strong mix of datasets!                                                                                                                                                                                                                                                                                                                             |
| [chatglm2-6b](https://github.com/THUDM/ChatGLM2-6B)                                     | æ¸…åå¤§å­¦&æ™ºè°±AI      | å¼€æº    | ChatGLM2-6B æ˜¯ChatGLM-6B çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œæ›´å¼ºå¤§çš„æ€§èƒ½ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä»2K æ‰©å±•åˆ°äº† 32Kï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼Œå…è®¸å•†ä¸šä½¿ç”¨ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [Baichuan-13B-Chat](https://github.com/baichuan-inc/Baichuan-13B)                       | ç™¾å·æ™ºèƒ½           | å¼€æº    | Baichuan-13B æ˜¯ç”±ç™¾å·æ™ºèƒ½ç»§ Baichuan-7B ä¹‹åå¼€å‘çš„åŒ…å« 130 äº¿å‚æ•°çš„å¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œåœ¨æƒå¨çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmark ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚                                                                                                                                                                                                                                                                                                                                                                                                                           |
| [vicuna-33b](https://github.com/lm-sys/FastChat)                                        | UCä¼¯å…‹åˆ©          | å¼€æº    | Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.                                                                                                                                                                                                                                                                                                                                                                                                              |
| [wizardlm-13b](https://github.com/nlpxucan/WizardLM)                                    | å¾®è½¯             | å¼€æº    | WizardLM: An Instruction-following LLM Using Evol-Instruct                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| [InternLM-Chat-7B](https://github.com/InternLM/InternLM/tree/main)                      | ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤      | å¼€æº    | ä½¿ç”¨ä¸Šä¸‡äº¿é«˜è´¨é‡è¯­æ–™ï¼Œå»ºç«‹æ¨¡å‹è¶…å¼ºçŸ¥è¯†ä½“ç³»ï¼›æ”¯æŒ8kè¯­å¢ƒçª—å£é•¿åº¦ï¼Œå®ç°æ›´é•¿è¾“å…¥ä¸æ›´å¼ºæ¨ç†ä½“éªŒï¼›é€šç”¨å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œæ”¯æŒç”¨æˆ·çµæ´»è‡ªåŠ©æ­å»ºæµç¨‹ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| [Llama-2-70b-chat](https://github.com/facebookresearch/llama)                           | meta           | å¼€æº    | Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM. |

